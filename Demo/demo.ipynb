{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"demo_sbd.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"2ys3I2reAOvD","colab_type":"text"},"source":["# For Everyone"]},{"cell_type":"markdown","metadata":{"id":"LKcBpIDcBF5T","colab_type":"text"},"source":["IMPORTS"]},{"cell_type":"code","metadata":{"id":"nwQ4Nmxgcmri","colab_type":"code","colab":{}},"source":["import argparse\n","import torch\n","import torchvision\n","from torch import optim\n","from torchvision import transforms\n","import os\n","import os.path as osp\n","import random\n","import numpy as np\n","from pathlib import Path\n","from torch.utils.data import dataset\n","import PIL\n","from PIL import Image\n","\n","# Fix seed to make experiments reproducibles\n","seed = 1\n","\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","np.random.seed(seed)\n","random.seed(seed)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8EAo5WN-79ds","colab_type":"text"},"source":["ACCESS TO THE DRIVE"]},{"cell_type":"code","metadata":{"id":"HLucYq9Loxgb","colab_type":"code","outputId":"4a20f297-9b2e-4f82-a40b-44367db3706c","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1562145138490,"user_tz":-120,"elapsed":26686,"user":{"displayName":"Sara Burrel Diez","photoUrl":"https://lh3.googleusercontent.com/-jgPQVT-8fDY/AAAAAAAAAAI/AAAAAAAAA8I/ALh6erGcc1Q/s64/photo.jpg","userId":"10057145515235458800"}}},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')\n","root_path = 'gdrive/My Drive/'  #change dir if necessary"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lpeNBZ61Bbee","colab_type":"text"},"source":["FOLDER WHERE THE WEIGHTS ARE STORED"]},{"cell_type":"code","metadata":{"id":"ZFg3godAt06I","colab_type":"code","colab":{}},"source":["# directory where we store the model weights\n","weights_dir = \"gdrive/My Drive/2019_AIDL_TEAM4/colab_face_detection_siamesa/Presentacio/Demo/Weights_here\" #change dir if necessary\n","if not osp.exists(weights_dir):\n","    os.mkdir(weights_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uSQ71Sz9AGa-","colab_type":"text"},"source":["# Demo"]},{"cell_type":"markdown","metadata":{"id":"wHn2r5ERptEg","colab_type":"text"},"source":["DEFINE ARGUMENTS"]},{"cell_type":"code","metadata":{"id":"qdqM3jDv9VTI","colab_type":"code","colab":{}},"source":["class Args:\n","\n","    frontal_images_directories = \"gdrive/My Drive/2019_AIDL_TEAM4/colab_face_detection_siamesa/dataset-cfp/Protocol/image_list_F.txt\" #change dir if necessary\n","    profile_images_directories = \"gdrive/My Drive/2019_AIDL_TEAM4/colab_face_detection_siamesa/dataset-cfp/Protocol/image_list_P.txt\" #change dir if necessary\n","    split_main_directory = \"gdrive/My Drive/2019_AIDL_TEAM4/colab_face_detection_siamesa/dataset-cfp/Protocol/Split\" #change dir if necessary\n","    split_traindata = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\"]\n","    split_valdata = [\"07\", \"08\"]\n","    split_testdata = [\"09\", \"10\"]\n","    dataset_root = \"gdrive/My Drive/2019_AIDL_TEAM4/colab_face_detection_siamesa\" #change dir if necessary\n","    dataset= \"CFPDataset\"\n","    lr = float(1e-4)\n","    weight_decay = float(0.0005)\n","    momentum = float(0.9)\n","    betas = (0.9, 0.999)\n","    batch_size = int(14)\n","    workers = int(8)\n","    start_epoch = int(0)\n","    epochs = int(40)\n","    pretrained = True\n","    data_aug = False\n","    resume = \"checkpoint_e63_40e_lr1e_3_SGD\"\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bHNIGrMcpw6C","colab_type":"text"},"source":["DEFINE DATASET CLASS"]},{"cell_type":"code","metadata":{"id":"b_gAtqmHsofp","colab_type":"code","colab":{}},"source":["\n","class CFPDataset(dataset.Dataset):\n","    def __init__(self, path, args, img_transforms=None, dataset_root=\"\",\n","                 split=\"train\", input_size=(224, 224)):\n","        super().__init__()\n","\n","        self.data = []\n","        self.split = split\n","\n","        self.load(path, args)\n","\n","        print(\"Dataset loaded\")\n","        print(\"{0} samples in the {1} dataset\".format(len(self.data),\n","                                                      self.split))\n","        self.transforms = img_transforms\n","        self.dataset_root = dataset_root\n","        self.input_size = input_size\n","\n","    def load(self, path, args):\n","\n","        # read directories for frontal images\n","        lines = open(args.frontal_images_directories).readlines()\n","        idx = 0\n","        directories_frontal_images = []\n","        while idx < len(lines):\n","            x = lines[idx].strip().split()\n","            directories_frontal_images.append(x)\n","            idx += 1\n","        \n","        # read directories for profile images\n","        lines = open(args.profile_images_directories).readlines()\n","        idx = 0\n","        directories_profile_images = []\n","        while idx < len(lines):\n","            x = lines[idx].strip().split()\n","            directories_profile_images.append(x)\n","            idx += 1\n","        \n","        # read same and different pairs of images and save at dictionary\n","        self.data = []\n","        for i in path:\n","            ff_diff_file = osp.join(args.split_main_directory, 'FF', i,\n","                                    'diff.txt')\n","            lines = open(ff_diff_file).readlines()\n","            idx = 0\n","            while idx < 3: #int(len(lines)/1): changed for inference demo\n","                img_pair = lines[idx].strip().split(',')\n","                img1_dir = directories_frontal_images[int(img_pair[0])-1][1]\n","                img2_dir = directories_frontal_images[int(img_pair[1])-1][1]\n","                pair_tag = -1\n","                d = {\n","                    \"img1_path\": img1_dir,\n","                    \"img2_path\": img2_dir,\n","                    \"pair_tag\": pair_tag\n","                }\n","                self.data.append(d)\n","                idx += 1\n","\n","            ff_same_file = osp.join(args.split_main_directory, 'FF', i,\n","                                    'same.txt')\n","            lines = open(ff_same_file).readlines()\n","            idx = 0\n","            while idx < 3: #int(len(lines)/1): changed for inference demo\n","                img_pair = lines[idx].strip().split(',')\n","                img1_dir = directories_frontal_images[int(img_pair[0])-1][1]\n","                img2_dir = directories_frontal_images[int(img_pair[1])-1][1]\n","                pair_tag = 1\n","                d = {\n","                    \"img1_path\": img1_dir,\n","                    \"img2_path\": img2_dir,\n","                    \"pair_tag\": pair_tag\n","                }\n","                self.data.append(d)\n","                idx += 1\n","\n","            fp_diff_file = osp.join(args.split_main_directory, 'FP', i,\n","                                    'diff.txt')\n","            lines = open(fp_diff_file).readlines()\n","            idx = 0\n","            while idx < 3: #int(len(lines)/1): changed for inference demo\n","                img_pair = lines[idx].strip().split(',')\n","                img1_dir = directories_frontal_images[int(img_pair[0])-1][1]\n","                img2_dir = directories_profile_images[int(img_pair[1])-1][1]\n","                pair_tag = -1\n","                d = {\n","                    \"img1_path\": img1_dir,\n","                    \"img2_path\": img2_dir,\n","                    \"pair_tag\": pair_tag\n","                }\n","                self.data.append(d)\n","                idx += 1\n","\n","            fp_same_file = osp.join(args.split_main_directory, 'FP', i,\n","                                    'same.txt')\n","            lines = open(fp_same_file).readlines()\n","            idx = 0\n","            while idx < 3: #int(len(lines)/1): changed for inference demo\n","                img_pair = lines[idx].strip().split(',')\n","                img1_dir = directories_frontal_images[int(img_pair[0])-1][1]\n","                img2_dir = directories_profile_images[int(img_pair[1])-1][1]\n","                pair_tag = 1\n","                d = {\n","                    \"img1_path\": img1_dir,\n","                    \"img2_path\": img2_dir,\n","                    \"pair_tag\": pair_tag\n","                }\n","                self.data.append(d)\n","                idx += 1\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        d = self.data[index]\n","        image1_path = osp.join(self.dataset_root, 'dataset-cfp', d[\n","            'img1_path'])\n","        image2_path = osp.join(self.dataset_root, 'dataset-cfp', d[\n","            'img2_path'])\n","        image1 = Image.open(image1_path).convert('RGB')\n","        image2 = Image.open(image2_path).convert('RGB')\n","        tag = d['pair_tag']\n","    \n","        if self.transforms is not None:\n","            # this converts from (HxWxC) to (CxHxW) as wel\n","            img1 = self.transforms(image1)\n","            img2 = self. transforms(image2)\n","\n","        return img1, img2, tag"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SvUKs18qD2g1","colab_type":"text"},"source":["DEFINE DATA LOADER"]},{"cell_type":"code","metadata":{"id":"OYIFlDuZpiAR","colab_type":"code","colab":{}},"source":["from torch.utils import data\n","\n","def get_dataloader(datapath, args, img_transforms=None, split=\"train\"):\n","\n","    if split == 'train':\n","        shuffle = True\n","        drop_last = True\n","    else:\n","        shuffle = False\n","        drop_last = False\n","    \n","    dataset = CFPDataset(datapath,\n","                         args,\n","                         split=split,\n","                         img_transforms=img_transforms,\n","                         dataset_root=osp.expanduser(args.dataset_root))\n","    \n","    data_loader = data.DataLoader(dataset,\n","                                  batch_size=args.batch_size,\n","                                  shuffle=shuffle,    \n","                                  num_workers=args.workers,\n","                                  pin_memory=True,\n","                                  drop_last=drop_last)\n","    return data_loader"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wOTmj-LtqZnX","colab_type":"text"},"source":["DEFINE MODEL"]},{"cell_type":"code","metadata":{"id":"OboRfyRMqbP8","colab_type":"code","colab":{}},"source":["\n","import torch\n","from torch import nn\n","from torchvision.models import resnext50_32x4d\n","\n","def l2norm(x):\n","  x = x / torch.sqrt(torch.sum(x**2, dim=-1, keepdim=True))\n","  return x\n","\n","class SiameseCosine(nn.Module):\n","    \"\"\"\n","    Siamese network\n","    \"\"\"\n","    def __init__(self, pretrained=False):\n","        super(SiameseCosine, self).__init__()\n","\n","        resnext50_32x4d_model = resnext50_32x4d(pretrained=pretrained)\n","        self.feat = resnext50_32x4d_model\n","        \n","    def forward(self, img1, img2):\n","        feat_1 = self.feat(img1)\n","        feat_1 = l2norm(feat_1)\n","        \n","        feat_2 = self.feat(img2)\n","        feat_2 = l2norm(feat_2)\n","\n","      \n","        return feat_1, feat_2\n","      "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mREx41lHEAMI","colab_type":"text"},"source":["DEFINE INFERENCE FUNCTIONS"]},{"cell_type":"code","metadata":{"id":"h1ankH7nKAMn","colab_type":"code","colab":{}},"source":["import torch\n","from torchvision import transforms\n","from torch.nn import functional as nnfunc\n","from IPython.display import display\n","from PIL import Image\n","import numpy as np\n","from torchvision.transforms import ToPILImage\n","\n","\n","def similarity (vec1, vec2):\n","    cos = torch.nn.CosineSimilarity(dim=1, eps=1e-8)\n","    cos_a = cos (vec1, vec2)  \n","    return cos_a\n","  \n","def show_results(img1, img2, vec1, vec2, y, treshold):\n"," \n","    similarity_value = similarity(vec1, vec2)\n","   \n","    for image1, image2, value, label in zip (img1, img2, similarity_value, y):\n","        imtype = np.uint8\n","        mean = 0.5\n","        std = 0.5\n","        if value > treshold: \n","            Predicted_label = 'Same Identity'\n","        else:\n","            Predicted_label = 'Different Identity'\n","        if label==1: \n","            Label = 'Same Identity'\n","        else:\n","            Label = 'Different Identity'\n","        print(\"\")\n","        print('Predicted label:', Predicted_label)\n","        print('Label:', Label)\n","        image1 = image1.cpu().float()\n","        image1 = (image1*std+mean)*255\n","        image1_numpy = image1.numpy()\n","        image1_numpy = np.transpose(image1_numpy, (1, 2, 0))\n","        display(Image.fromarray(image1_numpy.astype(imtype)))\n","        image2 = image2.cpu().float()\n","        image2 = (image2*std+mean)*255\n","        image2_numpy = image2.numpy()\n","        image2_numpy = np.transpose(image2_numpy, (1, 2, 0))\n","        display(Image.fromarray(image2_numpy.astype(imtype))) \n","        \n","    return \n","\n","def inference(model, dataloader, epoch, device, tr):\n","    model.eval()\n","        \n","    for idx, (img1, img2, prob) in enumerate(dataloader):\n","        img1 =  img1.to('cuda:0')\n","        img2 = img2.to('cuda:0')\n","        prob = prob.float().to('cuda:0')  #label\n","        \n","        out1, out2 = model(img1, img2) #inputs images to model, executes model, returns features\n","        \n","        show_results(img1, img2, out1, out2, prob, tr)\n","        \n","        torch.cuda.empty_cache()\n","    return"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QgRI2hIIEhn0","colab_type":"text"},"source":["STATE ARGUMENTS"]},{"cell_type":"code","metadata":{"id":"ZRnd7g8bEpyG","colab_type":"code","colab":{}},"source":["args = Args()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9HZKrv2pEWHd","colab_type":"text"},"source":["DEFINE IMAGE TRANSFORMATIONS"]},{"cell_type":"code","metadata":{"id":"8iZoeeWM9umb","colab_type":"code","colab":{}},"source":["train_transform=None\n","if args.data_aug == False:\n","  img_transforms = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n","\n","else:\n","  img_transforms = transforms.Compose([transforms.Resize((224, 224)), \n","                                        transforms.RandomHorizontalFlip(), \n","                                        transforms.RandomRotation(20, resample=PIL.Image.BILINEAR), \n","                                        transforms.ToTensor()])\n","\n","    \n","\n","val_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor()\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6JMvsxeIEtkI","colab_type":"text"},"source":["LOAD DATA FOR INFERENCE"]},{"cell_type":"code","metadata":{"id":"TRHfMSYutwAS","colab_type":"code","outputId":"ec983903-b83c-4f0d-ed37-84ddb23203c9","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1562145245127,"user_tz":-120,"elapsed":4862,"user":{"displayName":"Sara Burrel Diez","photoUrl":"https://lh3.googleusercontent.com/-jgPQVT-8fDY/AAAAAAAAAAI/AAAAAAAAA8I/ALh6erGcc1Q/s64/photo.jpg","userId":"10057145515235458800"}}},"source":["test_loader = get_dataloader(args.split_testdata, args,\n","                             img_transforms=val_transforms, split=\"test\")"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Dataset loaded\n","24 samples in the test dataset\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bLQRhestE3dk","colab_type":"text"},"source":["STATE DEVICE"]},{"cell_type":"code","metadata":{"id":"NJU0i3PpLAHm","colab_type":"code","outputId":"41bb73b6-c9c0-49fe-a662-1c6f4a1ee0dc","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1562145247335,"user_tz":-120,"elapsed":611,"user":{"displayName":"Sara Burrel Diez","photoUrl":"https://lh3.googleusercontent.com/-jgPQVT-8fDY/AAAAAAAAAAI/AAAAAAAAA8I/ALh6erGcc1Q/s64/photo.jpg","userId":"10057145515235458800"}}},"source":["torch.cuda.is_available()"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"X8NbSOen-wOy","colab_type":"code","colab":{}},"source":["# check for CUDA\n","if torch.cuda.is_available():\n","    device = torch.device('cuda:0')\n","else:\n","    device = torch.device('cpu')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FQVdj64jE6rE","colab_type":"text"},"source":["LOAD MODEL"]},{"cell_type":"code","metadata":{"id":"_cTc5vSct0U4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"069ddbf0-a150-4634-f789-4f4d309cc7a1","executionInfo":{"status":"ok","timestamp":1562145264418,"user_tz":-120,"elapsed":13664,"user":{"displayName":"Sara Burrel Diez","photoUrl":"https://lh3.googleusercontent.com/-jgPQVT-8fDY/AAAAAAAAAAI/AAAAAAAAA8I/ALh6erGcc1Q/s64/photo.jpg","userId":"10057145515235458800"}}},"source":["\n","model = SiameseCosine(pretrained=args.pretrained)\n","\n","model = model.to(device) \n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\" to /root/.cache/torch/checkpoints/resnext50_32x4d-7cdf4587.pth\n","100%|██████████| 100441675/100441675 [00:03<00:00, 26453508.71it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"PkD3z8eHE--G","colab_type":"text"},"source":["LOAD WEIGHTS"]},{"cell_type":"code","metadata":{"id":"XLhHSje0LRCb","colab_type":"code","outputId":"d6bc5a81-6acc-4270-f454-db5e6f4e3fe8","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1562145270925,"user_tz":-120,"elapsed":2811,"user":{"displayName":"Sara Burrel Diez","photoUrl":"https://lh3.googleusercontent.com/-jgPQVT-8fDY/AAAAAAAAAAI/AAAAAAAAA8I/ALh6erGcc1Q/s64/photo.jpg","userId":"10057145515235458800"}}},"source":["weights = osp.join(weights_dir,args.resume+'.pth')\n","epoch = 35 #extracted from experiment to reproduce\n","if args.resume:\n","    print(weights)\n","    checkpoint = torch.load(weights)\n","    model.load_state_dict(checkpoint['model'])\n","    # Set the start epoch if it has not been\n","    if not args.start_epoch:\n","        args.start_epoch = checkpoint['epoch']"],"execution_count":16,"outputs":[{"output_type":"stream","text":["gdrive/My Drive/2019_AIDL_TEAM4/colab_face_detection_siamesa/Presentacio/Demo/Weights_here/checkpoint_e63_40e_lr1e_3_SGD.pth\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DzC5wJEDFD7f","colab_type":"text"},"source":["EXECUTE INFERENCE"]},{"cell_type":"code","metadata":{"id":"EYZn9qBIuQ4O","colab_type":"code","outputId":"186f61ed-96f4-4657-fd27-9a42e2bda89c","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Ky1n4zM32LUrNi7kt8TXK69gYRXpKPwj"},"executionInfo":{"status":"ok","timestamp":1562145288303,"user_tz":-120,"elapsed":16178,"user":{"displayName":"Sara Burrel Diez","photoUrl":"https://lh3.googleusercontent.com/-jgPQVT-8fDY/AAAAAAAAAAI/AAAAAAAAA8I/ALh6erGcc1Q/s64/photo.jpg","userId":"10057145515235458800"}}},"source":["# Inference\n","\n","best_tr = 0.668 #extracted from experiment to reproduce\n","inference(model, test_loader, epoch, device=device, tr=best_tr)\n"],"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}