{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"siamesa_v1_sbd.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"WuSDsgCMEteW","colab_type":"text"},"source":["GENERAL IMPORTS AND SEED"]},{"cell_type":"code","metadata":{"id":"nwQ4Nmxgcmri","colab_type":"code","colab":{}},"source":["import argparse\n","import torch\n","import torchvision\n","from torch import optim\n","from torchvision import transforms\n","import os\n","import os.path as osp\n","import random\n","import numpy as np\n","from pathlib import Path\n","from torch.utils.data import dataset\n","import PIL\n","from PIL import Image\n","\n","# Fix the seed\n","seed = 1\n","\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","np.random.seed(seed)\n","random.seed(seed)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8EAo5WN-79ds","colab_type":"text"},"source":["ACCESS TO THE DRIVE FOLDER WHERE THE DATASET HAS BEEN STORED"]},{"cell_type":"code","metadata":{"id":"HLucYq9Loxgb","colab_type":"code","outputId":"fb6b8f26-3143-4fac-b5fb-823abc42048a","executionInfo":{"status":"ok","timestamp":1561217200473,"user_tz":-120,"elapsed":620,"user":{"displayName":"Sara Burrel Diez","photoUrl":"https://lh3.googleusercontent.com/-jgPQVT-8fDY/AAAAAAAAAAI/AAAAAAAAA8I/ALh6erGcc1Q/s64/photo.jpg","userId":"10057145515235458800"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')\n","root_path = 'gdrive/My Drive/2019_AIDL_TEAM4/colab_face_detection_siamesa/'  #change dir to your project folder!"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wHn2r5ERptEg","colab_type":"text"},"source":["DEFINE ARGUMENTS"]},{"cell_type":"code","metadata":{"id":"qdqM3jDv9VTI","colab_type":"code","colab":{}},"source":["class Args:\n","\n","    frontal_images_directories = \"gdrive/My Drive/2019_AIDL_TEAM4/colab_face_detection_siamesa/dataset-cfp/Protocol/image_list_F.txt\" #change dir !\n","    profile_images_directories = \"gdrive/My Drive/2019_AIDL_TEAM4/colab_face_detection_siamesa/dataset-cfp/Protocol/image_list_P.txt\" #change dir!\n","    split_main_directory = \"gdrive/My Drive/2019_AIDL_TEAM4/colab_face_detection_siamesa/dataset-cfp/Protocol/Split\" #change dir!\n","    split_traindata = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\"]\n","    split_valdata = [\"07\", \"08\"]\n","    split_testdata = [\"09\", \"10\"]\n","    dataset_root = \"gdrive/My Drive/2019_AIDL_TEAM4/colab_face_detection_siamesa\" #change dir!\n","    dataset= \"CFPDataset\"\n","    lr = float(1e-4)\n","    weight_decay = float(0.0005)\n","    momentum = float(0.9)\n","    betas = (0.9, 0.999)\n","    batch_size = int(14)\n","    workers = int(8)\n","    start_epoch = int(0)\n","    epochs = int(7)\n","    pretrained = True\n","    siamese_linear = True\n","    data_aug = True\n","    resume = \"checkpoint_e4\"\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bHNIGrMcpw6C","colab_type":"text"},"source":["DEFINE DATASET CLASS"]},{"cell_type":"code","metadata":{"id":"b_gAtqmHsofp","colab_type":"code","colab":{}},"source":["\n","class CFPDataset(dataset.Dataset):\n","    def __init__(self, path, args, img_transforms=None, dataset_root=\"\",\n","                 split=\"train\", input_size=(224, 224)):\n","        super().__init__()\n","\n","        self.data = []\n","        self.split = split\n","\n","        self.load(path, args)\n","\n","        print(\"Dataset loaded\")\n","        print(\"{0} samples in the {1} dataset\".format(len(self.data),\n","                                                      self.split))\n","        self.transforms = img_transforms\n","        self.dataset_root = dataset_root\n","        self.input_size = input_size\n","\n","    def load(self, path, args):\n","\n","        # read directories for frontal images\n","        lines = open(args.frontal_images_directories).readlines()\n","        idx = 0\n","        directories_frontal_images = []\n","        while idx < len(lines):\n","            x = lines[idx].strip().split()\n","            directories_frontal_images.append(x)\n","            idx += 1\n","        # read directories for profile images\n","        lines = open(args.profile_images_directories).readlines()\n","        idx = 0\n","        directories_profile_images = []\n","        while idx < len(lines):\n","            x = lines[idx].strip().split()\n","            directories_profile_images.append(x)\n","            idx += 1\n","        # read same and different pairs of images and save at dictionary\n","        self.data = []\n","        for i in path:\n","            ff_diff_file = osp.join(args.split_main_directory, 'FF', i,\n","                                    'diff.txt')\n","            lines = open(ff_diff_file).readlines()\n","            idx = 0\n","            while idx < int(len(lines)/1):\n","                img_pair = lines[idx].strip().split(',')\n","                img1_dir = directories_frontal_images[int(img_pair[0])-1][1]\n","                img2_dir = directories_frontal_images[int(img_pair[1])-1][1]\n","                pair_tag = 0.0\n","                d = {\n","                    \"img1_path\": img1_dir,\n","                    \"img2_path\": img2_dir,\n","                    \"pair_tag\": pair_tag\n","                }\n","                self.data.append(d)\n","                idx += 1\n","\n","            ff_same_file = osp.join(args.split_main_directory, 'FF', i,\n","                                    'same.txt')\n","            lines = open(ff_same_file).readlines()\n","            idx = 0\n","            while idx < int(len(lines)/1):\n","                img_pair = lines[idx].strip().split(',')\n","                img1_dir = directories_frontal_images[int(img_pair[0])-1][1]\n","                img2_dir = directories_frontal_images[int(img_pair[1])-1][1]\n","                pair_tag = 1.0\n","                d = {\n","                    \"img1_path\": img1_dir,\n","                    \"img2_path\": img2_dir,\n","                    \"pair_tag\": pair_tag\n","                }\n","                self.data.append(d)\n","                idx += 1\n","\n","            fp_diff_file = osp.join(args.split_main_directory, 'FP', i,\n","                                    'diff.txt')\n","            lines = open(fp_diff_file).readlines()\n","            idx = 0\n","            while idx < int(len(lines)/1):\n","                img_pair = lines[idx].strip().split(',')\n","                img1_dir = directories_frontal_images[int(img_pair[0])-1][1]\n","                img2_dir = directories_profile_images[int(img_pair[1])-1][1]\n","                pair_tag = 0.0\n","                d = {\n","                    \"img1_path\": img1_dir,\n","                    \"img2_path\": img2_dir,\n","                    \"pair_tag\": pair_tag\n","                }\n","                self.data.append(d)\n","                idx += 1\n","\n","            fp_same_file = osp.join(args.split_main_directory, 'FP', i,\n","                                    'same.txt')\n","            lines = open(fp_same_file).readlines()\n","            idx = 0\n","            while idx < int(len(lines)/1):\n","                img_pair = lines[idx].strip().split(',')\n","                img1_dir = directories_frontal_images[int(img_pair[0])-1][1]\n","                img2_dir = directories_profile_images[int(img_pair[1])-1][1]\n","                pair_tag = 1.0\n","                d = {\n","                    \"img1_path\": img1_dir,\n","                    \"img2_path\": img2_dir,\n","                    \"pair_tag\": pair_tag\n","                }\n","                self.data.append(d)\n","                idx += 1\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        d = self.data[index]\n","        image1_path = osp.join(self.dataset_root, 'dataset-cfp', d[\n","            'img1_path'])\n","        image2_path = osp.join(self.dataset_root, 'dataset-cfp', d[\n","            'img2_path'])\n","        image1 = Image.open(image1_path).convert('RGB')\n","        image2 = Image.open(image2_path).convert('RGB')\n","        tag = d['pair_tag']\n","        if self.transforms is not None:\n","            # this converts from (HxWxC) to (CxHxW) as wel\n","            img1 = self.transforms(image1)\n","            img2 = self. transforms(image2)\n","\n","        return img1, img2, tag"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tGMjd22gFRYA","colab_type":"text"},"source":["DEFINE DATA LOADER"]},{"cell_type":"code","metadata":{"id":"OYIFlDuZpiAR","colab_type":"code","colab":{}},"source":["from torch.utils import data\n","\n","def get_dataloader(datapath, args, img_transforms=None, split=\"train\"):\n","\n","    if split == 'train':\n","        shuffle = True\n","        drop_last = True\n","    else:\n","        shuffle = False\n","        drop_last = False\n","    \n","    dataset = CFPDataset(datapath,\n","                         args,\n","                         split=split,\n","                         img_transforms=img_transforms,\n","                         dataset_root=osp.expanduser(args.dataset_root))\n","    \n","    data_loader = data.DataLoader(dataset,\n","                                  batch_size=args.batch_size,\n","                                  shuffle=shuffle,    \n","                                  num_workers=args.workers,\n","                                  pin_memory=True,\n","                                  drop_last=drop_last)\n","    return data_loader"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wOTmj-LtqZnX","colab_type":"text"},"source":["MODEL"]},{"cell_type":"code","metadata":{"id":"OboRfyRMqbP8","colab_type":"code","colab":{}},"source":["import torch\n","from torch import nn\n","from torchvision.models import vgg16_bn\n","\n","\n","class SiameseDecision(nn.Module):\n","    \"\"\"\n","    Siamese network. Siamese_linear = False.\n","    \"\"\"\n","    def __init__(self, pretrained=False):\n","        super().__init__()\n","\n","        self.feat = vgg16_bn(pretrained=pretrained).features\n","\n","        self.fc1 = nn.Sequential(\n","            nn.Linear(in_features=512*7*7*2, out_features=4096),\n","            nn.ReLU(True),\n","            nn.Dropout()\n","        )\n","        self.fc2 = nn.Sequential(\n","            nn.Linear(in_features=4096, out_features=4096),\n","            nn.ReLU(True),\n","            nn.Dropout(),\n","            nn.Linear(in_features=4096, out_features=1) ,\n","        )\n","\n","    def forward(self, img1, img2):\n","        # the input to the vgg16 is a fixed-size 224x224 RGB image\n","        # we get the vgg16 features\n","        feat_1 = self.feat(img1)\n","        feat_2 = self.feat(img2)\n","        feat_1 = feat_1.view(feat_1.size(0), -1)\n","        feat_2 = feat_2.view(feat_2.size(0), -1)\n","        # we concatenate the two tensors of features\n","        feat = torch.cat((feat_1, feat_2), 1)\n","        # we run the classifier\n","        feat_3 = self.fc1(feat)\n","        tag = self.fc2(feat_3)\n","        return tag\n","\n","class SiameseLinearDecision(nn.Module):\n","    \"\"\"\n","    Siamese network. Siamese_linear = True.\n","    \"\"\"\n","    def __init__(self, pretrained=False):\n","        super().__init__()\n","\n","        self.feat = vgg16_bn(pretrained=pretrained).features\n","        \n","        self.fc1 = nn.Sequential(\n","            nn.Linear(in_features=512*7*7, out_features=4096),\n","            nn.ReLU(True),\n","            nn.Dropout()\n","        )\n","        self.fc2 = nn.Sequential(\n","            nn.Linear(in_features=4096*2, out_features=4096),\n","            nn.ReLU(True),\n","            nn.Dropout(),\n","            nn.Linear(in_features=4096, out_features=1) ,\n","        )\n","\n","    def forward(self, img1, img2):\n","        feat_1 = self.feat(img1)\n","        feat_2 = self.feat(img2)\n","        feat_1 = feat_1.view(feat_1.size(0), -1)\n","        feat_2 = feat_2.view(feat_2.size(0), -1)\n","        feat_1 = self.fc1(feat_1)\n","        feat_2 = self.fc1(feat_2)\n","       \n","        feat = torch.cat((feat_1, feat_2), 1)\n","        tag = self.fc2(feat)\n","        \n","        return tag"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I7A5_ZZdqf8Z","colab_type":"text"},"source":["DEFINE LOSS"]},{"cell_type":"code","metadata":{"id":"VLLRaLvFqit5","colab_type":"code","colab":{}},"source":["from torch import nn\n","\n","class RecognitionCriterion(nn.Module):\n","    def __init__(self):\n","        super().__init__() \n","        self.classification_criterion = nn.BCEWithLogitsLoss()\n","        self.cls_loss = None\n","\n","    def forward(self, *input):\n","        self.cls_loss = self.classification_criterion(*input)\n","        return self.cls_loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8c_WD2OCqnYa","colab_type":"text"},"source":["DEFINE TRAINING AND VALIDATION FUNCTIONS"]},{"cell_type":"code","metadata":{"id":"8c-eqydZqtKz","colab_type":"code","colab":{}},"source":["import torch\n","from torchvision import transforms\n","from torch.nn import functional as nnfunc\n","import numpy as np\n","\n","def accuracy(preds, y):\n","    preds = nnfunc.sigmoid(preds)\n","    correct = 0\n","    for p, label in zip(preds, y):\n","        if p > 0.5 and label == 1:\n","            correct += 1\n","        if p < 0.5 and label == 0:\n","            correct += 1\n","    return correct/preds.size(0)\n","\n","def train(model, loss_fn, optimizer, dataloader, epoch, device):\n","    model.train()\n","    all_loss = []\n","    all_acc = []\n","        \n","    for idx, (img1, img2, prob) in enumerate(dataloader):\n","        x1 = img1.float().to(device)\n","        x2 = img2.float().to(device)\n","        optimizer.zero_grad()\n","\n","        prob_t = prob.type(torch.FloatTensor)\n","        prob_var = prob_t.to(device)\n","        \n","        output = model(x1, x2)\n","        output = output.reshape(1, -1)\n","        output = output.squeeze()\n","        \n","        loss = loss_fn(output, prob_var)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        all_loss.append(loss.item())\n","        acc = accuracy(output, prob_var)\n","        all_acc.append(acc)             \n","        if idx % 14 == 0:\n","            message1 = \"TRAIN Epoch [{0}]: [{1}/{2}] \".format(epoch, idx,\n","                                                              len(dataloader))\n","            message2 = \"Loss: [{0:.4f}]; Accuracy: [{1}]\".format(loss.item(),\n","                                                                acc)\n","            print(message1, message2)\n","        torch.cuda.empty_cache()\n","    return all_loss, all_acc\n","\n","def val(model, loss_fn, dataloader, epoch, device):\n","    model.eval()\n","    all_loss = []\n","    all_acc =[]\n","    \n","    for idx, (img1, img2, prob) in enumerate(dataloader):\n","        x1 = img1.float().to(device)\n","        x2 = img2.float().to(device)\n","\n","        prob_t = prob.type(torch.FloatTensor)\n","        prob_var = prob_t.to(device)\n","        output = model(x1, x2)\n","        output = output.reshape(1, -1)\n","        output = output.squeeze()\n","        loss = loss_fn(output, prob_var)\n","        acc = accuracy(output, prob_var)\n","        all_loss.append(loss.item())\n","        all_acc.append(acc)\n","        \n","        if idx % 14 == 0:\n","            message1 = \"VAL Epoch [{0}]: [{1}/{2}] \".format(epoch, idx,\n","                                                              len(dataloader))\n","            message2 = \"Loss: [{0:.4f}]; Accuracy: [{1}]\".format(loss.item(), acc)\n","            print(message1, message2)\n","        torch.cuda.empty_cache()\n","    return all_loss, all_acc\n","  \n","def test(model, loss_fn, dataloader, epoch, device):\n","    model.eval()\n","    all_loss = []\n","    all_acc =[]\n","    \n","    for idx, (img1, img2, prob) in enumerate(dataloader):\n","        x1 = img1.float().to(device)\n","        x2 = img2.float().to(device)\n","\n","        prob_t = prob.type(torch.FloatTensor)\n","        prob_var = prob_t.to(device)\n","        output = model(x1, x2)\n","        output = output.reshape(1, -1)\n","        output = output.squeeze()\n","        loss = loss_fn(output, prob_var)\n","        acc = accuracy(output, prob_var)\n","        all_loss.append(loss.item())\n","        all_acc.append(acc)\n","        \n","        if idx % 14 == 0:\n","            message1 = \"TEST Epoch [{0}]: [{1}/{2}] \".format(epoch, idx,\n","                                                              len(dataloader))\n","            message2 = \"Loss: [{0:.4f}]; Accuracy: [{1}]\".format(loss.item(), acc)\n","            print(message1, message2)\n","        torch.cuda.empty_cache()\n","    return all_acc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y1KR-DckGV_n","colab_type":"text"},"source":["LOAD ARGUMENTS AND DEFINE IMAGE TRANSFORMATIONS"]},{"cell_type":"code","metadata":{"id":"8iZoeeWM9umb","colab_type":"code","colab":{}},"source":["args = Args()\n","\n","train_transform=None\n","if args.data_aug == False:\n","  img_transforms = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n","\n","else:\n","  img_transforms = transforms.Compose([transforms.Resize((224, 224)), \n","                                        transforms.RandomHorizontalFlip(), \n","                                        transforms.RandomRotation(20, resample=PIL.Image.BILINEAR), \n","                                        transforms.ToTensor()])\n","\n","    \n","\n","val_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor()\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vhX6XkOeFusP","colab_type":"text"},"source":["LOAD DATASET SPLIT FOR TRAINING"]},{"cell_type":"code","metadata":{"id":"yGzZEm6j94ML","colab_type":"code","outputId":"5171445f-256a-42cd-a807-c0a0ab0418fb","executionInfo":{"status":"ok","timestamp":1561217237992,"user_tz":-120,"elapsed":627,"user":{"displayName":"Sara Burrel Diez","photoUrl":"https://lh3.googleusercontent.com/-jgPQVT-8fDY/AAAAAAAAAAI/AAAAAAAAA8I/ALh6erGcc1Q/s64/photo.jpg","userId":"10057145515235458800"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["train_loader = get_dataloader(args.split_traindata, args,\n","                              img_transforms=img_transforms)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Dataset loaded\n","8400 samples in the train dataset\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GM6pUUXSF4ng","colab_type":"text"},"source":["LOAD DATASET SPLIT FOR VALIDATION"]},{"cell_type":"code","metadata":{"id":"FKQgt_NDrA5L","colab_type":"code","outputId":"fcea658b-515d-4974-a8b6-00579b28b980","executionInfo":{"status":"ok","timestamp":1561217240661,"user_tz":-120,"elapsed":577,"user":{"displayName":"Sara Burrel Diez","photoUrl":"https://lh3.googleusercontent.com/-jgPQVT-8fDY/AAAAAAAAAAI/AAAAAAAAA8I/ALh6erGcc1Q/s64/photo.jpg","userId":"10057145515235458800"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["val_loader = get_dataloader(args.split_valdata, args,\n","                            img_transforms=val_transforms, split=\"val\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Dataset loaded\n","2800 samples in the val dataset\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uGk7v792Gbk_","colab_type":"text"},"source":["SPECIFY DEVICE"]},{"cell_type":"code","metadata":{"id":"ERYf3wbPcmoX","colab_type":"code","outputId":"a6752373-fb27-4130-d535-ed0dbd8c2b84","executionInfo":{"status":"ok","timestamp":1561217245664,"user_tz":-120,"elapsed":600,"user":{"displayName":"Sara Burrel Diez","photoUrl":"https://lh3.googleusercontent.com/-jgPQVT-8fDY/AAAAAAAAAAI/AAAAAAAAA8I/ALh6erGcc1Q/s64/photo.jpg","userId":"10057145515235458800"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["torch.cuda.is_available()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"Z4y5Bk_xch4D","colab_type":"code","colab":{}},"source":["# check for CUDA\n","if torch.cuda.is_available():\n","    device = torch.device('cuda:0')\n","else:\n","    device = torch.device('cpu')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wbM3NZz_GlLi","colab_type":"text"},"source":["SELECT MODEL AND LOSS FUNCTION"]},{"cell_type":"code","metadata":{"id":"XXQ2coX6-Z_0","colab_type":"code","outputId":"0ed80648-5c33-42f5-f2ae-1eed60fe943f","executionInfo":{"status":"ok","timestamp":1561217282471,"user_tz":-120,"elapsed":8008,"user":{"displayName":"Sara Burrel Diez","photoUrl":"https://lh3.googleusercontent.com/-jgPQVT-8fDY/AAAAAAAAAAI/AAAAAAAAA8I/ALh6erGcc1Q/s64/photo.jpg","userId":"10057145515235458800"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["if args.siamese_linear == False:\n","  print('SiameseDecision')\n","  model = SiameseDecision(pretrained=args.pretrained)\n","else:\n","  print('SiameseLinearDecision')\n","  model = SiameseLinearDecision(pretrained=args.pretrained)\n","model = model.to(device) # treure de train i validation\n","\n","loss_fn = RecognitionCriterion()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["SiameseLinearDecision\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PPnnO-xVHmg-","colab_type":"text"},"source":["SPECIFY WEIGHTS DIRECTORY"]},{"cell_type":"code","metadata":{"id":"s0rOSRuG-mci","colab_type":"code","colab":{}},"source":["# directory where we'll store model weights\n","weights_dir = \"gdrive/My Drive/weights\" #change dir!\n","if not osp.exists(weights_dir):\n","  \n","    os.mkdir(weights_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fjoijLaFH0Dy","colab_type":"text"},"source":["SELECT OPTIMIZER"]},{"cell_type":"code","metadata":{"id":"kRW8oa-H-3gT","colab_type":"code","colab":{}},"source":["\n","optimizer = torch.optim.Adam(model.parameters(), lr=args.lr,\n","                             weight_decay=args.weight_decay)\n","\n","#optimizer = optim.SGD(model.parameters(), lr=args.lr,\n","#                      momentum=args.momentum, weight_decay=args.weight_decay)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FEuuuiHFH70g","colab_type":"text"},"source":["DEFINE CHECKOINT"]},{"cell_type":"code","metadata":{"id":"WAS5FOei-8T1","colab_type":"code","colab":{}},"source":["\n","#afegir acc i loss i fer que guardi el millor epoch\n","def save_checkpoint(state, filename=\"checkpoint.pth\", save_path=weights_dir):\n","    # check if the save directory exists\n","    if not Path(save_path).exists():\n","        Path(save_path).mkdir()\n","\n","    save_path = Path(save_path, filename)\n","    torch.save(state, str(save_path))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eCcMOLEBH_u1","colab_type":"text"},"source":["RUN TRAIN"]},{"cell_type":"code","metadata":{"id":"E0ddySP2_DPR","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","# train and evalute for `epochs`\n","loss_epoch_train = []\n","loss_epoch_val = []\n","acc_epoch_train = []\n","acc_epoch_val = []\n","\n","best_acc = -1\n","best_epoch = 0\n","\n","for epoch in range(args.start_epoch, args.epochs):\n","    train_loss, train_acc = train(model, loss_fn, optimizer, train_loader, epoch, device=device)\n","    \n","    av_loss = np.mean(train_loss)\n","    av_acc = np.mean(train_acc)\n","    loss_epoch_train.append(av_loss)\n","    acc_epoch_train.append(av_acc)\n","  \n","  \n","    val_loss, val_acc = val(model, loss_fn, val_loader, epoch, device=device)\n","    \n","    av_loss = np.mean(val_loss)\n","    av_acc = np.mean(val_acc)\n","    loss_epoch_val.append(av_loss)\n","    acc_epoch_val.append(av_acc)\n","    \n","    if best_acc < av_acc:\n","        best_acc = av_acc\n","        best_epoch = epoch\n","        save_checkpoint({\n","            'epoch': epoch + 1,\n","            'batch_size': val_loader.batch_size,\n","            'model': model.state_dict(),\n","            'optimizer': optimizer.state_dict()\n","         }, filename=str(args.resume)+\".pth\",\n","             save_path=weights_dir)\n","    \n","print(\"Best Epoch: \",best_epoch, \"Best Acc: \", best_acc)\n","    \n","epochs = range(1, len(loss_epoch_train) + 1)\n","# b is for \"solid blue line\"\n","plt.plot(epochs, loss_epoch_train, 'b', label='Training loss')\n","# r is for \"solid red line\"\n","plt.plot(epochs, loss_epoch_val, 'r', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n","\n","epochs = range(1, len(acc_epoch_train) + 1)\n","# b is for \"solid blue line\"\n","plt.plot(epochs, acc_epoch_train, 'b', label='Training accuracy')\n","# r is for \"solid red line\"\n","plt.plot(epochs, acc_epoch_val, 'r', label='Validation accuracy')\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SFXe_xBVIJ1_","colab_type":"text"},"source":["LOAD DATASET SPLIT FOR TEST"]},{"cell_type":"code","metadata":{"id":"cQJO6F0rQKB2","colab_type":"code","outputId":"a6e25b2a-1a72-4ed9-d73e-baaedcc1f91a","executionInfo":{"status":"ok","timestamp":1561217242702,"user_tz":-120,"elapsed":733,"user":{"displayName":"Sara Burrel Diez","photoUrl":"https://lh3.googleusercontent.com/-jgPQVT-8fDY/AAAAAAAAAAI/AAAAAAAAA8I/ALh6erGcc1Q/s64/photo.jpg","userId":"10057145515235458800"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["test_loader = get_dataloader(args.split_testdata, args,\n","                             img_transforms=val_transforms, split=\"test\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Dataset loaded\n","2800 samples in the test dataset\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5Uvo8bUwIPkw","colab_type":"text"},"source":["LOAD STORED WEIGHTS FOR BEST EPOCH"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7c9ISfwIY1YL","outputId":"1b50a6d7-208d-4138-8c2f-d00200bd6a6a","executionInfo":{"status":"ok","timestamp":1561150944721,"user_tz":-120,"elapsed":3999,"user":{"displayName":"Sara Burrel Diez","photoUrl":"https://lh3.googleusercontent.com/-jgPQVT-8fDY/AAAAAAAAAAI/AAAAAAAAA8I/ALh6erGcc1Q/s64/photo.jpg","userId":"10057145515235458800"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["args.resume = osp.join(weights_dir+args.resume)\n","epoch = 4\n","if args.resume:\n","    print(args.resume)\n","    checkpoint = torch.load(args.resume)\n","    model.load_state_dict(checkpoint['model'])\n","    # Set the start epoch if it has not been\n","    if not args.start_epoch:\n","        args.start_epoch = checkpoint['epoch']"],"execution_count":0,"outputs":[{"output_type":"stream","text":["weights/checkpoint_4.pth\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XR5yo2zRIXO-","colab_type":"text"},"source":["RUN TEST"]},{"cell_type":"code","metadata":{"id":"jTpaEgq0SrxM","colab_type":"code","colab":{}},"source":["# Test\n","test_acc = test(model, loss_fn, test_loader, epoch, device=device)\n","\n","av_acc = np.mean(test_acc)\n","print('Average test accuracy:', av_acc)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XLhHSje0LRCb","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}